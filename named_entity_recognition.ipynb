{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uUtFa4-o_rM6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "import numpy\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLjGBS-CIBJL",
        "outputId": "21e0ac8e-8ed1-47c6-ea4c-9f6133cae3b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd0IJ5JtfHuV",
        "outputId": "5ad8b033-ef26-4779-a4fd-a90f559e7dc1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=64"
      ],
      "metadata": {
        "id": "d3WEQtYQUOOo"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preporation\n",
        "\n"
      ],
      "metadata": {
        "id": "chDtLwWg_2u5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokinizer"
      ],
      "metadata": {
        "id": "knP5P4Z3SQ3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokinizerUtil():\n",
        "  def __init__(self, maxSquanceLength=25, maxVocabalaryLength=10000):\n",
        "    self.dictonary={\"PAD\": 0, \"UNK\": 1}\n",
        "    self.maxVocabalaryLength=maxVocabalaryLength\n",
        "    self.maxSquanceLength=maxSquanceLength\n",
        "\n",
        "  def addWord(self, word:str) -> None:\n",
        "    if self.maxVocabalaryLength>len(self.dictonary):\n",
        "      if word not in self.dictonary:\n",
        "        self.dictonary[word]=len(self.dictonary)\n",
        "\n",
        "  def findWordsToken(self, vocabalary:dict, word:str) -> int:\n",
        "    if word in vocabalary:\n",
        "      return vocabalary[word]\n",
        "    return vocabalary[\"UNK\"]\n",
        "\n",
        "  def pad(self, message:list[int]):\n",
        "    paddingRequired=self.maxSquanceLength-len(message)\n",
        "    paddingList=[self.dictonary[\"PAD\"]]*paddingRequired\n",
        "\n",
        "    return message+paddingList\n",
        "\n",
        "  def findTokensWord(self, vocabalary:dict, token:int) -> str:\n",
        "    if token in vocabalary:\n",
        "      return vocabalary[token]\n",
        "    return vocabalary[1]"
      ],
      "metadata": {
        "id": "9OfdcSK9SUQ5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokinizer(TokinizerUtil):\n",
        "  def encode(self, message:str):\n",
        "    encodedMessage=[]\n",
        "\n",
        "    for word in message.split(\" \"):\n",
        "      if len(encodedMessage)>=self.maxSquanceLength:\n",
        "        return self.pad(encodedMessage)\n",
        "\n",
        "      self.addWord(word)\n",
        "      token=self.findWordsToken(self.dictonary, word)\n",
        "      encodedMessage.append(token)\n",
        "\n",
        "    return self.pad(encodedMessage)\n",
        "\n",
        "  def decode(self, encodedMessage:list[int]):\n",
        "    decodedMessage=\"\"\n",
        "    decodeHash={v: k for k, v in self.dictonary.items()}\n",
        "\n",
        "    for token in encodedMessage:\n",
        "      word=self.findTokensWord(decodeHash, token)\n",
        "      decodedMessage+=word+\" \"\n",
        "\n",
        "    return decodedMessage\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.dictonary)"
      ],
      "metadata": {
        "id": "sDXJmdZ0Sh3B"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "5acd7K4cSOxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetUtil(Dataset):\n",
        "  def selectColumns(self, df:pd.core.frame.DataFrame, *args:str):\n",
        "    columns=[]\n",
        "    for columnName in args:\n",
        "      columns.append(list(df[columnName]))\n",
        "\n",
        "    return columns\n",
        "\n",
        "  def reformatData(self, words:list[str], tags:list[str], tokinizer:Tokinizer) -> list[list[str]]:\n",
        "    resultSentance = []\n",
        "    resultTag = []\n",
        "\n",
        "    tempSentence = []\n",
        "    tempTag = []\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "      tempSentence.append(word)\n",
        "      tempTag.append(tags[i])\n",
        "\n",
        "      if word == \".\":\n",
        "        resultSentance.append(tempSentence)\n",
        "        resultTag.append(tempTag)\n",
        "        tempSentence = []\n",
        "        tempTag = []\n",
        "\n",
        "    return resultSentance, resultTag\n"
      ],
      "metadata": {
        "id": "RMWl1nDOMsdi"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(DatasetUtil):\n",
        "  def __init__(self, csvFilePath:str):\n",
        "    self.wordTokinizer=Tokinizer()\n",
        "    tagTokinizer=Tokinizer()\n",
        "\n",
        "    pandasDf=pd.read_csv(csvFilePath, encoding='latin1')\n",
        "    columns=self.selectColumns(pandasDf, \"Word\", \"Tag\")\n",
        "\n",
        "    words, tags=columns[0], columns[1]\n",
        "    resultSentance, resultTag = self.reformatData(words, tags, self.wordTokinizer)\n",
        "\n",
        "    processedSentance, processedTag = self.encode(resultSentance, resultTag, self.wordTokinizer)\n",
        "\n",
        "    self.sentanceTensor=torch.tensor(processedSentance)\n",
        "    self.tagTensor=torch.tensor(processedTag)\n",
        "\n",
        "  def encode(self, resultSentance, resultTag, wordTokinizer):\n",
        "    processedSentance=[]\n",
        "    processedTag=[]\n",
        "    for index in range(len(resultSentance)):\n",
        "      sentance = \" \".join(resultSentance[index])\n",
        "      tag = \" \".join(resultTag[index])\n",
        "\n",
        "      processedSentance.append(wordTokinizer.encode(sentance))\n",
        "      processedTag.append(wordTokinizer.encode(tag))\n",
        "\n",
        "    return processedSentance, processedTag\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.sentanceTensor[index], self.tagTensor[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentanceTensor)\n",
        "\n",
        "\n",
        "dataset = CustomDataset(\"/content/drive/MyDrive/ner_dataset.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnAeIT_RK-tw",
        "outputId": "f1d55a07-7f6c-4e92-8492-be81363b4f00"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.wordTokinizer:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DataLoader"
      ],
      "metadata": {
        "id": "jSrwZChJT6hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataloader():\n",
        "  def __init__(self, dataset, trainSize: float):\n",
        "    self.dataset=dataset\n",
        "    self.trainSize=trainSize\n",
        "\n",
        "  def trainTestDataloader(self):\n",
        "    trainDataset, testDataset=self.splitDataset()\n",
        "\n",
        "    trainDataloader=DataLoader(trainDataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "    testDataloader=DataLoader(testDataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "    return trainDataloader, testDataloader\n",
        "\n",
        "  def splitDataset(self):\n",
        "    trainSize = int(self.trainSize * len(self.dataset))\n",
        "    testSize = len(self.dataset) - trainSize\n",
        "    trainDataset, testDataset = torch.utils.data.random_split(self.dataset, [trainSize, testSize])\n",
        "\n",
        "    return trainDataset, testDataset"
      ],
      "metadata": {
        "id": "wqNAvSnmT6Ml"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "CX6dYL3eUweL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NERModel(nn.Module):\n",
        "  def __init__(self, vocabalarySize, hiddenSize):\n",
        "    super().__init__()\n",
        "    self.sequential=nn.Sequential(\n",
        "      nn.Embedding(vocabalarySize, hiddenSize),\n",
        "      nn.LSTM(hiddenSize, hiddenSize),\n",
        "    )\n",
        "\n",
        "    self.output=nn.Sequential(\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hiddenSize, vocabalarySize),\n",
        "      nn.Softmax(1)\n",
        "    )\n",
        "  def forward(self, sentance):\n",
        "    x,_=self.sequential(sentance)\n",
        "    return self.output(x).argmax(2)\n",
        "\n",
        "nerModel=NERModel(len(dataset.wordTokinizer.dictonary), 25)\n"
      ],
      "metadata": {
        "id": "KDm0oVsWUyTB"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizer & Loss"
      ],
      "metadata": {
        "id": "2MU9RDRBcysd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(nerModel.parameters(), lr=0.001)\n",
        "loss=nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "UAV_e02kc3V_"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "KOrhNjqXVjZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainig():\n",
        "  def __init__(self, dataset, epochs, model, loss, optimizer, device):\n",
        "    self.trainDataloader, self.testDataloader = Dataloader(dataset, .8).trainTestDataloader()\n",
        "\n",
        "    self.epochs = epochs\n",
        "    self.model = model\n",
        "    self.loss = loss\n",
        "    self.optimizer = optimizer\n",
        "    self.device=device\n",
        "\n",
        "    self.trainingLoop()\n",
        "\n",
        "  def trainingLoop(self):\n",
        "    for epoch in range(self.epochs):\n",
        "      trainGenerator=self.train()\n",
        "      trainLoss, trainAccuracy=self.unpackGenerator(trainGenerator)\n",
        "\n",
        "      testGenerator=self.test()\n",
        "      testLoss, testAccuracy=self.unpackGenerator(testGenerator)\n",
        "\n",
        "      print(f\"train acc: {trainAccuracy:.2f}, train loss, {trainLoss:.2f} | test acc: {testAccuracy:.2f}, test loss, {testLoss:.2f}\")\n",
        "\n",
        "  def train(self):\n",
        "    for input, target in self.trainDataloader:\n",
        "      prediction = self.model(input)\n",
        "\n",
        "      loss, accuracy=self.getLossAndAccuracy(prediction, target)\n",
        "      loss.requires_grad = True\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      yield loss, accuracy\n",
        "\n",
        "\n",
        "  def test(self):\n",
        "    for input, target in self.testDataloader:\n",
        "      prediction = self.model(input)\n",
        "      loss, accuracy=self.getLossAndAccuracy(prediction, target)\n",
        "\n",
        "      yield loss, accuracy\n",
        "\n",
        "  def unpackGenerator(self, generator)->torch.tensor:\n",
        "    generator=next(iter(generator))\n",
        "    loss, accuracy=generator[0],generator[1]\n",
        "    return loss, accuracy\n",
        "\n",
        "  def getLossAndAccuracy(self,prediction,target)->torch.tensor:\n",
        "    prediction=prediction.float().to(self.device)\n",
        "    target=target.float().to(self.device)\n",
        "\n",
        "    prediction_loss=self.loss(prediction,target)\n",
        "    prediction_acc=self.accuracy(prediction,target)\n",
        "\n",
        "    return prediction_loss,prediction_acc\n",
        "\n",
        "  def accuracy(self,predictions,targets)->torch.tensor:\n",
        "    assert predictions.shape == targets.shape, \"Shapes of predictions and targets must match.\"\n",
        "\n",
        "    num_correct = (predictions == targets).sum().item()\n",
        "\n",
        "    total_samples = targets.numel()\n",
        "    accuracy_value = num_correct / total_samples\n",
        "    return accuracy_value*100\n",
        "\n",
        "Trainig(dataset, 1000, nerModel, loss, optimizer, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv1fRWAHZpyt",
        "outputId": "39187d04-68f4-4d36-f43f-b2bbd6ec8f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train acc: 0.00, train loss, 2800062.00 | test acc: 0.00, test loss, 3175264.50\n",
            "train acc: 0.00, train loss, 3013414.50 | test acc: 0.00, test loss, 2709776.25\n",
            "train acc: 0.00, train loss, 3266611.50 | test acc: 0.00, test loss, 2975950.50\n",
            "train acc: 0.00, train loss, 3485143.50 | test acc: 0.00, test loss, 3632354.25\n",
            "train acc: 0.00, train loss, 3148631.00 | test acc: 0.00, test loss, 3368162.75\n",
            "train acc: 0.00, train loss, 3255262.25 | test acc: 0.00, test loss, 3067940.00\n",
            "train acc: 0.00, train loss, 2773809.25 | test acc: 0.00, test loss, 3085017.75\n",
            "train acc: 0.00, train loss, 2994151.00 | test acc: 0.00, test loss, 4192210.75\n",
            "train acc: 0.00, train loss, 2604864.50 | test acc: 0.00, test loss, 3286033.50\n",
            "train acc: 0.00, train loss, 3206524.00 | test acc: 0.00, test loss, 3260446.50\n",
            "train acc: 0.00, train loss, 3453101.50 | test acc: 0.00, test loss, 3201768.50\n",
            "train acc: 0.00, train loss, 2955147.75 | test acc: 0.00, test loss, 3081964.25\n",
            "train acc: 0.00, train loss, 3353851.00 | test acc: 0.00, test loss, 2762863.50\n"
          ]
        }
      ]
    }
  ]
}